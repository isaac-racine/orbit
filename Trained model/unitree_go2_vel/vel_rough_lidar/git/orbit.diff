--- git status ---
On branch alix
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/extensions/omni.isaac.orbit/omni/isaac/orbit/envs/mdp/rewards.py
	modified:   source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/__init__.py
	modified:   source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/velocity/config/unitree_go2/agents/rsl_rl_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/position/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/extensions/omni.isaac.orbit/omni/isaac/orbit/envs/mdp/rewards.py b/source/extensions/omni.isaac.orbit/omni/isaac/orbit/envs/mdp/rewards.py
index 8d27ee6..4114762 100644
--- a/source/extensions/omni.isaac.orbit/omni/isaac/orbit/envs/mdp/rewards.py
+++ b/source/extensions/omni.isaac.orbit/omni/isaac/orbit/envs/mdp/rewards.py
@@ -21,7 +21,7 @@ from omni.isaac.orbit.managers.manager_term_cfg import RewardTermCfg
 from omni.isaac.orbit.sensors import ContactSensor
 
 if TYPE_CHECKING:
-    from omni.isaac.orbit.envs import RLTaskEnv
+	from omni.isaac.orbit.envs import RLTaskEnv
 
 """
 General.
@@ -29,43 +29,43 @@ General.
 
 
 def is_alive(env: RLTaskEnv) -> torch.Tensor:
-    """Reward for being alive."""
-    return (~env.termination_manager.terminated).float()
+	"""Reward for being alive."""
+	return (~env.termination_manager.terminated).float()
 
 
 def is_terminated(env: RLTaskEnv) -> torch.Tensor:
-    """Penalize terminated episodes that don't correspond to episodic timeouts."""
-    return env.termination_manager.terminated.float()
+	"""Penalize terminated episodes that don't correspond to episodic timeouts."""
+	return env.termination_manager.terminated.float()
 
 
 class is_terminated_term(ManagerTermBase):
-    """Penalize termination for specific terms that don't correspond to episodic timeouts.
+	"""Penalize termination for specific terms that don't correspond to episodic timeouts.
 
-    The parameters are as follows:
+	The parameters are as follows:
 
-    * attr:`term_keys`: The termination terms to penalize. This can be a string, a list of strings
-      or regular expressions. Default is ".*" which penalizes all terminations.
+	* attr:`term_keys`: The termination terms to penalize. This can be a string, a list of strings
+	  or regular expressions. Default is ".*" which penalizes all terminations.
 
-    The reward is computed as the sum of the termination terms that are not episodic timeouts.
-    This means that the reward is 0 if the episode is terminated due to an episodic timeout. Otherwise,
-    if two termination terms are active, the reward is 2.
-    """
+	The reward is computed as the sum of the termination terms that are not episodic timeouts.
+	This means that the reward is 0 if the episode is terminated due to an episodic timeout. Otherwise,
+	if two termination terms are active, the reward is 2.
+	"""
 
-    def __init__(self, cfg: RewardTermCfg, env: RLTaskEnv):
-        # initialize the base class
-        super().__init__(cfg, env)
-        # find and store the termination terms
-        term_keys = cfg.params.get("term_keys", ".*")
-        self._term_names = env.termination_manager.find_terms(term_keys)
+	def __init__(self, cfg: RewardTermCfg, env: RLTaskEnv):
+		# initialize the base class
+		super().__init__(cfg, env)
+		# find and store the termination terms
+		term_keys = cfg.params.get("term_keys", ".*")
+		self._term_names = env.termination_manager.find_terms(term_keys)
 
-    def __call__(self, env: RLTaskEnv, term_keys: str | list[str] = ".*") -> torch.Tensor:
-        # Return the unweighted reward for the termination terms
-        reset_buf = torch.zeros(env.num_envs, device=env.device)
-        for term in self._term_names:
-            # Sums over terminations term values to account for multiple terminations in the same step
-            reset_buf += env.termination_manager.get_term(term)
+	def __call__(self, env: RLTaskEnv, term_keys: str | list[str] = ".*") -> torch.Tensor:
+		# Return the unweighted reward for the termination terms
+		reset_buf = torch.zeros(env.num_envs, device=env.device)
+		for term in self._term_names:
+			# Sums over terminations term values to account for multiple terminations in the same step
+			reset_buf += env.termination_manager.get_term(term)
 
-        return (reset_buf * (~env.termination_manager.time_outs)).float()
+		return (reset_buf * (~env.termination_manager.time_outs)).float()
 
 
 """
@@ -74,47 +74,47 @@ Root penalties.
 
 
 def lin_vel_z_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize z-axis base linear velocity using L2-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.square(asset.data.root_lin_vel_b[:, 2])
+	"""Penalize z-axis base linear velocity using L2-kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	return torch.square(asset.data.root_lin_vel_b[:, 2])
 
 
 def ang_vel_xy_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize xy-axis base angular velocity using L2-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)
+	"""Penalize xy-axis base angular velocity using L2-kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	return torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)
 
 
 def flat_orientation_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize non-flat base orientation using L2-kernel.
+	"""Penalize non-flat base orientation using L2-kernel.
 
-    This is computed by penalizing the xy-components of the projected gravity vector.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
+	This is computed by penalizing the xy-components of the projected gravity vector.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	return torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
 
 
 def base_height_l2(
-    env: RLTaskEnv, target_height: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+	env: RLTaskEnv, target_height: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
 ) -> torch.Tensor:
-    """Penalize asset height from its target using L2-kernel.
+	"""Penalize asset height from its target using L2-kernel.
 
-    Note:
-        Currently, it assumes a flat terrain, i.e. the target height is in the world frame.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # TODO: Fix this for rough-terrain.
-    return torch.square(asset.data.root_pos_w[:, 2] - target_height)
+	Note:
+		Currently, it assumes a flat terrain, i.e. the target height is in the world frame.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	# TODO: Fix this for rough-terrain.
+	return torch.square(asset.data.root_pos_w[:, 2] - target_height)
 
 
 def body_lin_acc_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize the linear acceleration of bodies using L2-kernel."""
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.norm(asset.data.body_lin_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)
+	"""Penalize the linear acceleration of bodies using L2-kernel."""
+	asset: Articulation = env.scene[asset_cfg.name]
+	return torch.sum(torch.norm(asset.data.body_lin_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)
 
 
 """
@@ -123,88 +123,88 @@ Joint penalties.
 
 
 def joint_torques_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint torques applied on the articulation using L2-kernel.
+	"""Penalize joint torques applied on the articulation using L2-kernel.
 
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint torques contribute to the L2 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.applied_torque[:, asset_cfg.joint_ids]), dim=1)
+	NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint torques contribute to the L2 norm.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	return torch.sum(torch.square(asset.data.applied_torque[:, asset_cfg.joint_ids]), dim=1)
 
 
 def joint_vel_l1(env: RLTaskEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize joint velocities on the articulation using an L1-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
+	"""Penalize joint velocities on the articulation using an L1-kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	return torch.sum(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
 
 
 def joint_vel_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint velocities on the articulation using L1-kernel.
+	"""Penalize joint velocities on the articulation using L1-kernel.
 
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint velocities contribute to the L1 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
+	NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint velocities contribute to the L1 norm.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	return torch.sum(torch.square(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
 
 
 def joint_acc_l2(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint accelerations on the articulation using L2-kernel.
+	"""Penalize joint accelerations on the articulation using L2-kernel.
 
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint accelerations contribute to the L2 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.joint_acc[:, asset_cfg.joint_ids]), dim=1)
+	NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint accelerations contribute to the L2 norm.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	return torch.sum(torch.square(asset.data.joint_acc[:, asset_cfg.joint_ids]), dim=1)
 
 
 def joint_deviation_l1(env, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint positions that deviate from the default one."""
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    angle = asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]
-    return torch.sum(torch.abs(angle), dim=1)
+	"""Penalize joint positions that deviate from the default one."""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	# compute out of limits constraints
+	angle = asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]
+	return torch.sum(torch.abs(angle), dim=1)
 
 
 def joint_pos_limits(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint positions if they cross the soft limits.
-
-    This is computed as a sum of the absolute value of the difference between the joint position and the soft limits.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    out_of_limits = -(
-        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 0]
-    ).clip(max=0.0)
-    out_of_limits += (
-        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 1]
-    ).clip(min=0.0)
-    return torch.sum(out_of_limits, dim=1)
+	"""Penalize joint positions if they cross the soft limits.
+
+	This is computed as a sum of the absolute value of the difference between the joint position and the soft limits.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	# compute out of limits constraints
+	out_of_limits = -(
+		asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 0]
+	).clip(max=0.0)
+	out_of_limits += (
+		asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 1]
+	).clip(min=0.0)
+	return torch.sum(out_of_limits, dim=1)
 
 
 def joint_vel_limits(
-    env: RLTaskEnv, soft_ratio: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+	env: RLTaskEnv, soft_ratio: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
 ) -> torch.Tensor:
-    """Penalize joint velocities if they cross the soft limits.
+	"""Penalize joint velocities if they cross the soft limits.
 
-    This is computed as a sum of the absolute value of the difference between the joint velocity and the soft limits.
+	This is computed as a sum of the absolute value of the difference between the joint velocity and the soft limits.
 
-    Args:
-        soft_ratio: The ratio of the soft limits to be used.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    out_of_limits = (
-        torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids])
-        - asset.data.soft_joint_vel_limits[:, asset_cfg.joint_ids] * soft_ratio
-    )
-    # clip to max error = 1 rad/s per joint to avoid huge penalties
-    out_of_limits = out_of_limits.clip_(min=0.0, max=1.0)
-    return torch.sum(out_of_limits, dim=1)
+	Args:
+		soft_ratio: The ratio of the soft limits to be used.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	# compute out of limits constraints
+	out_of_limits = (
+		torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids])
+		- asset.data.soft_joint_vel_limits[:, asset_cfg.joint_ids] * soft_ratio
+	)
+	# clip to max error = 1 rad/s per joint to avoid huge penalties
+	out_of_limits = out_of_limits.clip_(min=0.0, max=1.0)
+	return torch.sum(out_of_limits, dim=1)
 
 
 """
@@ -213,32 +213,32 @@ Action penalties.
 
 
 def applied_torque_limits(env: RLTaskEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize applied torques if they cross the limits.
+	"""Penalize applied torques if they cross the limits.
 
-    This is computed as a sum of the absolute value of the difference between the applied torques and the limits.
+	This is computed as a sum of the absolute value of the difference between the applied torques and the limits.
 
-    .. caution::
-        Currently, this only works for explicit actuators since we manually compute the applied torques.
-        For implicit actuators, we currently cannot retrieve the applied torques from the physics engine.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    # TODO: We need to fix this to support implicit joints.
-    out_of_limits = torch.abs(
-        asset.data.applied_torque[:, asset_cfg.joint_ids] - asset.data.computed_torque[:, asset_cfg.joint_ids]
-    )
-    return torch.sum(out_of_limits, dim=1)
+	.. caution::
+		Currently, this only works for explicit actuators since we manually compute the applied torques.
+		For implicit actuators, we currently cannot retrieve the applied torques from the physics engine.
+	"""
+	# extract the used quantities (to enable type-hinting)
+	asset: Articulation = env.scene[asset_cfg.name]
+	# compute out of limits constraints
+	# TODO: We need to fix this to support implicit joints.
+	out_of_limits = torch.abs(
+		asset.data.applied_torque[:, asset_cfg.joint_ids] - asset.data.computed_torque[:, asset_cfg.joint_ids]
+	)
+	return torch.sum(out_of_limits, dim=1)
 
 
 def action_rate_l2(env: RLTaskEnv) -> torch.Tensor:
-    """Penalize the rate of change of the actions using L2-kernel."""
-    return torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1)
+	"""Penalize the rate of change of the actions using L2-kernel."""
+	return torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1)
 
 
 def action_l2(env: RLTaskEnv) -> torch.Tensor:
-    """Penalize the actions using L2-kernel."""
-    return torch.sum(torch.square(env.action_manager.action), dim=1)
+	"""Penalize the actions using L2-kernel."""
+	return torch.sum(torch.square(env.action_manager.action), dim=1)
 
 
 """
@@ -247,25 +247,25 @@ Contact sensor.
 
 
 def undesired_contacts(env: RLTaskEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize undesired contacts as the number of violations that are above a threshold."""
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # check if contact force is above threshold
-    net_contact_forces = contact_sensor.data.net_forces_w_history
-    is_contact = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold
-    # sum over contacts for each environment
-    return torch.sum(is_contact, dim=1)
+	"""Penalize undesired contacts as the number of violations that are above a threshold."""
+	# extract the used quantities (to enable type-hinting)
+	contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+	# check if contact force is above threshold
+	net_contact_forces = contact_sensor.data.net_forces_w_history
+	is_contact = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold
+	# sum over contacts for each environment
+	return torch.sum(is_contact, dim=1)
 
 
 def contact_forces(env: RLTaskEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize contact forces as the amount of violations of the net contact force."""
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    net_contact_forces = contact_sensor.data.net_forces_w_history
-    # compute the violation
-    violation = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] - threshold
-    # compute the penalty
-    return torch.sum(violation.clip(min=0.0), dim=1)
+	"""Penalize contact forces as the amount of violations of the net contact force."""
+	# extract the used quantities (to enable type-hinting)
+	contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
+	net_contact_forces = contact_sensor.data.net_forces_w_history
+	# compute the violation
+	violation = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] - threshold
+	# compute the penalty
+	return torch.sum(violation.clip(min=0.0), dim=1)
 
 
 """
@@ -274,25 +274,50 @@ Velocity-tracking rewards.
 
 
 def track_lin_vel_xy_exp(
-    env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+	env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
 ) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # compute the error
-    lin_vel_error = torch.sum(
-        torch.square(env.command_manager.get_command(command_name)[:, :2] - asset.data.root_lin_vel_b[:, :2]),
-        dim=1,
-    )
-    return torch.exp(-lin_vel_error / std**2)
+	"""Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	# compute the error
+	lin_vel_error = torch.sum(
+		torch.square(env.command_manager.get_command(command_name)[:, :2] - asset.data.root_lin_vel_b[:, :2]),
+		dim=1,
+	)
+	return torch.exp(-lin_vel_error / std**2)
+
+
+def track_pos_xy_exp(
+	env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+) -> torch.Tensor:
+	"""Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	# compute the error
+	pos_error = torch.sum(
+		torch.square(env.command_manager.get_command(command_name)[:, :3]),
+		dim=1,
+	)
+	return torch.exp(-pos_error / std**2)
 
 
 def track_ang_vel_z_exp(
-    env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+	env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
+) -> torch.Tensor:
+	"""Reward tracking of position commands (yaw) using exponential kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	# compute the error
+	ang_vel_error = torch.square(env.command_manager.get_command(command_name)[:, 2] - asset.data.root_ang_vel_b[:, 2])
+	return torch.exp(-ang_vel_error / std**2)
+
+
+def track_heading_exp(
+	env: RLTaskEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
 ) -> torch.Tensor:
-    """Reward tracking of angular velocity commands (yaw) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # compute the error
-    ang_vel_error = torch.square(env.command_manager.get_command(command_name)[:, 2] - asset.data.root_ang_vel_b[:, 2])
-    return torch.exp(-ang_vel_error / std**2)
+	"""Reward tracking of heading commands (yaw) using exponential kernel."""
+	# extract the used quantities (to enable type-hinting)
+	asset: RigidObject = env.scene[asset_cfg.name]
+	# compute the error
+	heading_error = torch.square(env.command_manager.get_command(command_name)[:, 3])
+	return torch.exp(-heading_error / std**2)
diff --git a/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/__init__.py b/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/__init__.py
index 54fe12a..1d64c5c 100644
--- a/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/__init__.py
+++ b/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/__init__.py
@@ -6,3 +6,4 @@
 """Locomotion environments for legged robots."""
 
 from .velocity import *  # noqa
+from .position import *  
\ No newline at end of file
diff --git a/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/velocity/config/unitree_go2/agents/rsl_rl_cfg.py b/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/velocity/config/unitree_go2/agents/rsl_rl_cfg.py
index 7de557b..8e7f961 100644
--- a/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/velocity/config/unitree_go2/agents/rsl_rl_cfg.py
+++ b/source/extensions/omni.isaac.orbit_tasks/omni/isaac/orbit_tasks/locomotion/velocity/config/unitree_go2/agents/rsl_rl_cfg.py
@@ -15,8 +15,8 @@ from omni.isaac.orbit_tasks.utils.wrappers.rsl_rl import (
 @configclass
 class UnitreeGo2RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
     num_steps_per_env = 24
-    max_iterations = 1500
-    save_interval = 50
+    max_iterations = 10000
+    save_interval = 500
     experiment_name = "unitree_go2_rough"
     empirical_normalization = False
     policy = RslRlPpoActorCriticCfg(